services:
  mlflow:
    image: ghcr.io/mlflow/mlflow:v2.2.2
    ports:
      - "5000:5000"
    environment:
      - MLFLOW_BACKEND_STORE_URI=sqlite:///mlflow.db
      - MLFLOW_ARTIFACT_ROOT=/mlruns
    volumes:
      - ./mlruns:/mlruns
    command: mlflow server --backend-store-uri sqlite:///mlflow.db --default-artifact-root /mlruns --host 0.0.0.0

  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000"
    volumes:
      - ./data/processed:/app/data/processed
      - ./mlruns:/mlruns
    depends_on:
      - mlflow

  # Dummy service to build the preprocessing image
  preprocessing:
    build:
      context: .
      dockerfile: Dockerfile.preprocessing
    image: weather_australia_preprocessing:latest
    entrypoint: ["echo", "preprocessing image built"]

  # Dummy service to build the modelling image
  modelling:
    build:
      context: .
      dockerfile: Dockerfile.modelling
    image: weather_australia_modelling:latest
    entrypoint: ["echo", "modelling image built"]
    volumes:
      - ./mlruns:/mlruns

  airflow:
    image: apache/airflow:2.9.1
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - HOST_PROJECT_PATH=${HOST_PROJECT_PATH}
      - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-docker
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./data:/app/data
      - airflow_db:/opt/airflow
      - //var/run/docker.sock:/var/run/docker.sock
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    image: apache/airflow:2.9.1
    environment:
      - AIRFLOW__CORE__EXECUTOR=SequentialExecutor
      - AIRFLOW__CORE__FERNET_KEY=${AIRFLOW__CORE__FERNET_KEY}
      - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=False
      - AIRFLOW__CORE__LOAD_EXAMPLES=False
      - HOST_PROJECT_PATH=${HOST_PROJECT_PATH}
      - _PIP_ADDITIONAL_REQUIREMENTS=apache-airflow-providers-docker
    volumes:
      - ./dags:/opt/airflow/dags
      - ./data:/opt/airflow/data
      - ./data:/app/data
      - airflow_db:/opt/airflow
      - //var/run/docker.sock:/var/run/docker.sock
    depends_on:
      - airflow
    command: scheduler

volumes:
  airflow_db: